{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Difference Between AWS Regions, Availability Zones, and Edge Locations\n",
        "- Term\t          ------------Description\n",
        " - AWS                              - Region\tA physical location in the world with multiple, isolated Availability Zones (e.g., us-east-1, eu-west-1).\n",
        " - Availability Zone (AZ)\t          - A data center or group of data centers within a region. They offer high availability by being isolated from failures.\n",
        " - Edge Location                     -\tCDN endpoints used by Amazon\n",
        "\n",
        "\n",
        "- Importance for Data Analytics & Low-Latency Apps:\n",
        "\n",
        "\n",
        "    - Deploying close to users (Edge) improves performance.\n",
        "\n",
        "    - Using multiple AZs ensures high availability.\n",
        "\n",
        "    - Choosing the right Region ensures compliance (e.g., GDPR in EU).\n",
        "\n"
      ],
      "metadata": {
        "id": "UvtPJT32lO5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja47hQDRkcPF",
        "outputId": "9e24652f-01e1-4fb9-a88c-bb17e7058a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: aws: command not found\n",
            "/bin/bash: line 1: aws: command not found\n",
            "/bin/bash: line 1: aws: command not found\n"
          ]
        }
      ],
      "source": [
        "#2.\n",
        "!aws ec2 describe-regions --query \"Regions[*].RegionName\" --output tabl\n",
        "#2\n",
        "#AWS CLI: List All Available AWS Regions\n",
        "!aws ec2 describe-regions --query \"Regions[*].RegionName\" --output table\n",
        "#Output (sample):| RegionName   |\n",
        "#|--------------|\n",
        "#| us-east-1    |\n",
        "#| us-west-1    |\n",
        "#2\n",
        "#AWS CLI: List All Available AWS Regions\n",
        "!aws ec2 describe-regions --query \"Regions[*].RegionName\" --output table\n",
        "#Output (sample):| RegionName   |\n",
        "#|--------------|\n",
        "#| us-east-1    |\n",
        "#| us-west-1    |\n",
        "#| eu-central-1 |\n",
        "#| ap-south-1   |\n",
        "#| us-east-1    |\n",
        "#| us-west-1    |\n",
        "#| eu-central-1 |\n",
        "#| ap-south-1   |\n",
        "\n",
        "\n",
        "#| ap-south-1   |\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"s3:ListBucket\",\n",
        "        \"s3:GetObject\",\n",
        "        \"s3:PutObject\"\n",
        "      ],\n",
        "      \"Resource\": [\n",
        "        \"arn:aws:s3:::your-bucket-name\",\n",
        "        \"arn:aws:s3:::your-bucket-name/*\"\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQIj390-nXVB",
        "outputId": "31e80855-62cd-403e-834e-106fd6d9aa1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Version': '2012-10-17',\n",
              " 'Statement': [{'Effect': 'Allow',\n",
              "   'Action': ['s3:ListBucket', 's3:GetObject', 's3:PutObject'],\n",
              "   'Resource': ['arn:aws:s3:::your-bucket-name',\n",
              "    'arn:aws:s3:::your-bucket-name/*']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Amazon S3 Storage Class Comparison\n",
        "\n",
        "Storage Class\t           Use Case\t           Retrieval time        COST\n",
        "Standard           Frequently accessed data   Milliseconds  $$(highest\n",
        "Intelligent-Tiering\tUnknown access patterns; auto-tiering\tMilliseconds\t$$\n",
        "Glacier\tArchival data; infrequent access\tMinutes to hrs\t$\n",
        "\n",
        "‚úÖ Best Practice:\n",
        "\n",
        "Use Standard for active analytics.\n",
        "\n",
        "Use Intelligent-Tiering for unpredictable usage.\n",
        "\n",
        "Use Glacier for archived logs or compliance storage."
      ],
      "metadata": {
        "id": "qLEUTJYznpxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. RDS vs DynamoDB vs Redshift in Data Pipelines\n",
        "Service\tType\tBest For\tExample Use Case\n",
        "Amazon RDS\tRelational (SQL)\tStructured transactional data\tStore user info or metadata for analytics apps\n",
        "Amazon DynamoDB\tNoSQL (Key-Value)\tReal-time, high-speed lookups\tTrack user behavior or logs in dashboards\n",
        "Amazon Redshift\tData Warehouse\tLarge-scale analytical queries\tOLAP queries on sales, customer, product data\n",
        "\n",
        "6. What is Serverless Computing? Pros & Cons of AWS Lambda\n",
        "üîç Definition:\n",
        "Serverless computing abstracts infrastructure management. You deploy code and AWS automatically handles provisioning, scaling, and maintenance.\n",
        "\n",
        "‚úÖ Pros:\n",
        "No server management\n",
        "\n",
        "Auto scaling\n",
        "\n",
        "Pay only for usage\n",
        "\n",
        "Easy to connect with AWS services\n",
        "\n",
        "‚ùå Cons:\n",
        "Cold start latency\n",
        "\n",
        "Limited runtime and execution time\n",
        "\n",
        "Limited debugging\n",
        "\n",
        "Not ideal for long-running ETL jobs\n",
        "\n",
        "‚úÖ In Data Pipelines:\n",
        "AWS Lambda is great for:\n",
        "\n",
        "Lightweight transformations\n",
        "\n",
        "Triggers (e.g., on S3 upload)\n",
        "\n",
        "Real-time notifications\n",
        "\n",
        "7. Difference Between Kinesis Data Streams, Firehose, and Analytics\n",
        "Service\tDescription\tUse Case\n",
        "Kinesis Data Streams\tReal-time stream capture (DIY processing)\tCustom app to process sensor or log data\n",
        "Kinesis Firehose\tManaged delivery stream ‚Üí S3/Redshift\tReal-time ETL to S3 or Redshift\n",
        "Kinesis Data Analytics\tSQL-based stream analytics\tAnalyze clickstream in real-time\n",
        "\n",
        "8. Columnar Storage and Redshift Performance\n",
        "üîç Columnar Storage:\n",
        "Stores data column-by-column rather than row-by-row.\n",
        "\n",
        "‚úÖ Benefits in Redshift:\n",
        "Reduces I/O for analytic queries (only reads relevant columns)\n",
        "\n",
        "Enables better compression\n",
        "\n",
        "Improves query performance for large data scans\n",
        "\n",
        "‚úÖ Why Important:\n",
        "For OLAP-style queries like SELECT SUM(sales) FROM orders, columnar access dramatically reduces read time and improves speed"
      ],
      "metadata": {
        "id": "6dMk_ToJojiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. S3 Bucket Creation and Versioning\n",
        "\n",
        "Created bucket: analytics-demo-bucket\n",
        "\n",
        "Enabled versioning via S3 console.\n",
        "\n",
        "Uploaded sales.csv twice with changes to show 2 versions.\n",
        "\n",
        "Screenshot Placeholder: s3_versioning_screenshot.png\n"
      ],
      "metadata": {
        "id": "QRmhqHvPqoOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Lifecycle Policy for Glacier + Deletion\n",
        "\n",
        "#Policy JSON:\n",
        "{\n",
        "  \"Rules\": [\n",
        "    {\n",
        "      \"ID\": \"MoveToGlacierAndDelete\",\n",
        "      \"Status\": \"Enabled\",\n",
        "      \"Filter\": {\"Prefix\": \"\"},\n",
        "      \"Transitions\": [\n",
        "        {\n",
        "          \"Days\": 30,\n",
        "          \"StorageClass\": \"GLACIER\"\n",
        "        }\n",
        "      ],\n",
        "      \"Expiration\": {\n",
        "        \"Days\": 90\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkWTmqSbnlsi",
        "outputId": "674f3d6d-15d5-48f5-cbe5-13aa6e3a9d8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Rules': [{'ID': 'MoveToGlacierAndDelete',\n",
              "   'Status': 'Enabled',\n",
              "   'Filter': {'Prefix': ''},\n",
              "   'Transitions': [{'Days': 30, 'StorageClass': 'GLACIER'}],\n",
              "   'Expiration': {'Days': 90}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. DynamoDB Table and Lambda Trigger\n",
        "\n",
        "#Table: CustomerData\n",
        "\n",
        "#Primary key: customer_id\n",
        "\n",
        "#Inserted 3 manual records.\n",
        "\n",
        "#Lambda Code: (S3-triggered\n",
        "# Install boto3 if it's not already installed\n",
        "!pip install boto3\n",
        "import json\n",
        "import boto3\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    dynamodb = boto3.resource('dynamodb')\n",
        "    table = dynamodb.Table('CustomerData')\n",
        "    for record in event['Records']:\n",
        "        table.put_item(Item={\n",
        "            'customer_id': record['s3']['object']['key'],\n",
        "            'timestamp': record['eventTime']\n",
        "        })\n",
        "        return 'Successfully processed {} records.'.format(len(event['Records']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEtDSBdIq2ph",
        "outputId": "4df7a6a3-89b7-4280-e35a-146e09610272"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.38.23-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore<1.39.0,>=1.38.23 (from boto3)\n",
            "  Downloading botocore-1.38.23-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.23->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.23->boto3) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.23->boto3) (1.17.0)\n",
            "Downloading boto3-1.38.23-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.38.23-py3-none-any.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.38.23 botocore-1.38.23 jmespath-1.0.1 s3transfer-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#4. Lambda Logging to CloudWatch\n",
        "\n",
        "#Trigger: S3 upload\n",
        "\n",
        "#Logs filename, size, and timestamp\n",
        "\n",
        "#Code Snippet:\n",
        "import json\n",
        "import logging\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    for record in event['Records']:\n",
        "        print(\"File Uploaded:\", record['s3']['object']['key'])\n",
        "        print(\"Size:\", record['s3']['object']['size'])\n",
        "        print(\"Timestamp:\", record['eventTime'])\n",
        "\n",
        "        #Screenshot Placeholder: cloudwatch_logs_upload.png"
      ],
      "metadata": {
        "id": "2eNes50ysZyq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Crawled S3 bucket ‚Üí Created table in Glue Data Catalog\n",
        "\n",
        "# Ran job to convert sales.csv to sales.parquet\n",
        "\n",
        "#Glue Job Script:\n",
        "\n",
        "dyf = glueContext.create_dynamic_frame.from_catalog(database=\"demo_db\", table_name=\"sales_csv\")\n",
        "dyf_parquet = dyf.repartition(1)\n",
        "    frame = dyf_parquet,\n",
        "    connection_type = \"s3\",\n",
        "    connection_options = {\"path\": \"s3://analytics-demo-bucket/output/\"},\n",
        "    format = \"parquet\"\n",
        "\n",
        ")\n",
        "CREATE TABLE sales (\n",
        "    id INT,\n",
        "    date DATE,\n",
        "    amount FLOAT\n",
        ");\n",
        "COPY sales FROM 's3://analytics-demo-bucket/sales.csv'\n",
        "CREDENTIALS 'aws_iam_role=arn:aws:iam::account-id:role/RedshiftRole'\n",
        "CSV;\n",
        "SELECT * FROM \"demo_db\".\"sales_csv\" LIMIT 10;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "4aP-sbNstlBi",
        "outputId": "06e415dd-4e05-4f15-8706-e45c40387284"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-35-816d58cb5fca>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-816d58cb5fca>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    frame = dyf_parquet,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "End-to-End Data Analytics Pipeline on AWS\n",
        "This pipeline demonstrates a common pattern for batch processing and analytics on AWS.\n",
        "\n",
        "mermaid\n",
        "graph LR\n",
        "    A[Data Source] --> B(Ingestion: Kinesis Firehose);\n",
        "    B --> C(Storage: S3 Data Lake);\n",
        "    C --> D(Transformation: Glue ETL);\n",
        "    D --> E(Analytics Data Store: Redshift);\n",
        "    D --> F(NoSQL Data Store: DynamoDB);\n",
        "    E --> G(Visualization/BI: QuickSight);\n",
        "    F --> G;\n",
        "    C --> H(Ad-hoc Querying: Athena);\n",
        "    H --> G;\n",
        "Use code with caution\n",
        "Pipeline Stages and Service Choices\n",
        "Data Ingestion: Kinesis Firehose\n",
        "\n",
        "Why Kinesis Firehose? Kinesis Firehose is a fully managed service for reliably loading streaming data into data lakes, data stores, and analytics services. It's ideal for this stage because it simplifies the process of capturing, transforming, and loading streaming data without requiring you to manage infrastructure. It automatically scales to handle varying data volumes and can batch and compress data before sending it to the destination, optimizing for cost and downstream processing.\n",
        "How it's used: Data streams from various sources (applications, IoT devices, logs) are sent to Kinesis Firehose. Firehose automatically buffers the data and delivers it to the designated S3 bucket.\n",
        "Data Storage: S3 Data Lake\n",
        "\n",
        "Why S3? Amazon S3 (Simple Storage Service) is an object storage service offering industry-leading scalability, data availability, security, and performance. It's the foundational layer of a data lake due to its low cost, durability, and ability to store structured, semi-structured, and unstructured data in its raw format. This allows for schema-on-read flexibility later in the pipeline.\n",
        "How it's used: Kinesis Firehose delivers the raw incoming data directly into specified folders within an S3 bucket. This raw data layer serves as the single source of truth for the analytics pipeline.\n",
        "Data Transformation: Glue ETL\n",
        "\n",
        "Why Glue ETL? AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. It's serverless, meaning you don't need to manage servers. Glue Data Catalog can automatically discover and catalog metadata from the S3 data lake, providing a unified metadata repository. Glue ETL jobs (written in Python or Scala) can read data from S3, perform transformations (cleaning, joining, aggregating, changing format, partitioning), and write the transformed data to another location, such as a curated layer in S3 or directly into Redshift.\n",
        "How it's used: A Glue Crawler can scan the raw data in S3 and populate the Glue Data Catalog with table definitions. A Glue ETL job reads data from the raw S3 layer (using the Data Catalog), applies necessary transformations (e.g., converting CSV to Parquet or ORC for better query performance, filtering data, enriching records), and writes the clean, transformed data to a curated S3 layer or directly loads it into Redshift.\n",
        "Analytics Data Store: Redshift\n",
        "\n",
        "Why Redshift? Amazon Redshift is a fully managed, petabyte-scale data warehouse service. It's optimized for large-scale analytical queries (OLAP - Online Analytical Processing) on structured and semi-structured data. Redshift uses columnar storage and parallel processing, making it significantly faster than traditional row-based databases for analytical workloads. It's ideal for powering dashboards and business intelligence tools.\n",
        "How it's used: The transformed data from the Glue ETL job can be loaded into Redshift for fast querying and reporting by BI tools. Redshift Spectrum can also be used to query data directly in S3 without loading it, providing flexibility.\n",
        "NoSQL Data Store (for specific use cases): DynamoDB\n",
        "\n",
        "Why DynamoDB? Amazon DynamoDB is a fast and flexible NoSQL database service for single-digit millisecond performance at any scale. While not the primary store for broad analytics, it's excellent for specific use cases requiring low-latency key-value lookups, such as storing session data, user profiles, feature flags, or real-time dashboards powered by pre-aggregated data.\n",
        "How it's used: Data processed by Lambda or Glue might update or populate DynamoDB for real-time applications or specific dashboard widgets that require fast access to individual items.\n",
        "Ad-hoc Querying: Athena\n",
        "\n",
        "Why Athena? Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. It's serverless, and you pay only for the queries you run. Athena works directly with the Glue Data Catalog and is perfect for data analysts or data scientists who need to explore data in S3 without needing to load it into a data warehouse.\n",
        "How it's used: Analysts can use Athena to query the raw or transformed data stored in S3 using SQL, making it easy to perform ad-hoc analysis or explore data quality before loading into Redshift.\n",
        "Visualization/Business Intelligence: QuickSight\n",
        "\n",
        "Why QuickSight? Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud. It allows you to easily create and publish interactive dashboards, reports, and analytics. QuickSight can connect directly to various AWS data sources, including Redshift, Athena, S3, and DynamoDB (via other services if needed), making it a natural choice for visualizing the results of your data pipeline.\n",
        "How it's used: QuickSight connects to Redshift (for curated analytical data), Athena (for querying data in S3), or potentially DynamoDB (for specific real-time data points) to build dashboards and visualizations that provide business insights.\n",
        "This pipeline provides a robust, scalable, and cost-effective way to handle batch data analytics workloads on AWS, leveraging managed services to reduce operational overhead."
      ],
      "metadata": {
        "id": "jUjzF-jDv9MD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pmiakz7Uvbwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MfJ67omXvbvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}